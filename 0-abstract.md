# Co‑aligning AI and Institutions

# **Abstract**

Artificial‑intelligence alignment cannot be solved by focusing on single systems in a vacuum. Even perfectly intent‑aligned models will go awry when embedded inside misaligned economic, political, or social institutions. We argue that two dominant paradigms—(i) preference/utility maximisation inherited from the Standard Institution Design Toolkit (SIDT) and (ii) prompt‑ or self‑critique–based "text alignment"—are structurally incapable of delivering robust socio‑technical alignment. Instead we propose Full‑Stack Alignment (FSA): a new toolkit centred on explicit, structured representations of human norms and values. After analysing the shortcomings of existing toolkits, we outline two fundamental approaches to norm and value representation: (1) defining attractor points toward which values and norms naturally trend, such as Kantian equilibria and self-other generalizations, and (2) tightening the structure of what counts as a norm or value, as exemplified by constitutive attentional policies. We illustrate the promise of these approaches through five case studies ranging from AI value stewardship to democratic regulation. We close with a research and deployment roadmap toward institutions and AI systems that co‑evolve for global human flourishing.
