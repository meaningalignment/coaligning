# Abstract {.unnumbered .unlisted}

AI alignment cannot be solved by focusing on a single system in isolation. Even perfectly intent-aligned AI embedded in misaligned institutions will lead to dangerous outcomes. The emerging field of "sociotechnical alignment" is currently undermined by reliance on two inadequate paradigms: the "Standard Institution Design Toolkit" (SIDT)—preference/utility frameworks from economics and game theory—and naive value-representations without structural guarantees. Both fail to account for moral reasoning, value evolution, and social context. Through five application areas—AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions—we demonstrate why these thin models of human rationality cannot address our sociotechnical alignment challenges. We propose a new toolkit of thick models of choice that makes previously intractable problems tractable and enables co-alignment of AI and institutions toward global human flourishing. We call this "Full-Stack Alignment" (FSA), referring to the co-alignment of AI systems and institutions across the entire societal stack using thick models of choice that explicitly represent values and norms. Finally, we present a Full Stack Alignment Research Proposal for moving from theoretical foundations to practical adoption.