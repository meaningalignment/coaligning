# We need thick models of choice to co‑align AI and Institutions

# **Abstract**

AI alignment cannot be solved by focusing on a single system in isolation. Even perfectly intent-aligned AI embedded in misaligned institutions will lead to dangerous outcomes. The emerging field of "sociotechnical alignment" is currently undermined by reliance on two inadequate paradigms: the "Standard Institution Design Toolkit" (SIDT)—preference/utility frameworks from economics and game theory—and unstructured textual representations of values and norms. Both fail to account for moral reasoning, value evolution, and social context. Through five case studies—AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions—we demonstrate why these thin models of human rationality cannot address our sociotechnical alignment challenges. We propose a new toolkit of thick models of choice that makes previously intractable problems tractable and enables co-alignment of AI and institutions toward global human flourishing. We call this "Full-Stack Alignment". Finally, we present a concrete research and implementation plan for moving full-stack alignment from theory to implementation.