# Introduction

The growing field of **socio‑technical alignment** begins from a simple observation: beneficial outcomes cannot be guaranteed by aligning *individual* AI systems with their operators’ intentions.  Real‑world AI always lands inside larger institutions—corporations, markets, states, professional communities—whose incentive structures may oppose, distort, or outright negate those intentions.  A recommendation engine tuned to maximise *engagement*, for instance, may faithfully optimise the metric handed to it and still drive users toward value collapse; a trading bot that follows the letter of financial regulation may nonetheless exploit the spirit of market rules; an LLM “ambassador” representing a company can conclude treaties in milliseconds long before democratic oversight can react.  In each case, local alignment is washed out by misalignment in the surrounding institutional fabric.

Recognising this, many researchers have reached for the **Standard Institution Design Toolkit (SIDT)**—the family of models from micro‑economics, game theory, mechanism design, welfare economics, and social choice that idealises all agents, human or artificial, as *rational utility maximisers*.  Others have turned to the opposite extreme: **naive value representations** that encode norms as ad‑hoc blocks of natural language (prompts, constitutions, policy specs) and rely on an AI’s emergent interpretive abilities.  Unfortunately, *both* paths inherit blind spots that become fatal once AI begins to steer high‑stakes institutions:

* **SIDT misses moral plasticity and shared meaning.** By treating preferences as fixed, private orderings over outcomes, it cannot tell authentic desires from ones shaped by manipulation, addiction, or power asymmetries.  A preference‑maximising model happily optimises users into compulsive scrolling because the revealed data say “they like it.”  In multi‑agent contexts, SIDT’s equilibrium notions leave no principled room for the *reasons* people accept norms—only for pay‑off‑driven coordination.  The result is systems that look rational yet prove normatively incompetent: they cooperate when incentives align, defect when they don’t, and never notice the higher goods that hold communities together.

* **Naive textual specifications forfeit verifiability and resilience.** Lists of constitutional principles (“be helpful,” “address historical injustice,” “promote fun”) are easy to author but impossible to audit with any guarantee.  The same sentence can license divergent behaviours in different contexts, and frantic prompt‑patching after each failure creates a moving target.  Worse, such free‑form inputs act as magnets for ideological slogans—"Defund the Police," "Family Values"—smuggling tribal badges into alignment targets and exposing models to social‑media‑scale pressure campaigns.

Taken together, these deficiencies foreshadow a suite of **near‑term socio‑technical failures**: value collapse, normative brittleness, Machiavellian negotiation spirals, economies that prosper while humans languish, and democratic regulation becoming too slow to keep pace.  Each looming failure points not to a need for ever‑smarter optimisation, but to the demand for **thicker models of choice**—frameworks that take substantive stances on both the content and form of human values rather than treating all preferences or instructions as equally valid.

The remainder of this paper develops such a framework.  We call it **Full‑Stack Alignment (FSA)**: the project of co‑aligning AI systems *and* the institutions that embed them by replacing thin utility curves and free‑form value words with structured, accountable models of what humans find worth caring about.  Section 2 diagnoses in depth the six limitations of SIDT and the three vulnerabilities of naive value text. Section 3 introduces two complementary design strategies—one focused on what values fundamentally aim at, and another on how they should be structured and reasoned with—and
  shows how these approaches can be folded back into familiar economic and machine‑learning machinery. Sections 4.1 to 4.5 walk through five application areas where FSA turns previously intractable alignment problems into tractable engineering questions, and Section 5 sketches a research roadmap from foundational theory to flagship implementations.

In short, if we want AI that *augments* rather than erodes human flourishing, we must move beyond the comfort of SIDT’s optimisation calculus and the convenience of textual constitutions.  We need thick, explicit models of choice that let both machines and institutions see—and be held accountable to—the values we refuse to trade away.

