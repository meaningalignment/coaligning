## 2.1 Inadequacy of Utility Function and Preference-Based Approaches

To address the challenge of socio‑technical alignment we must rethink how institutions are designed and evaluated. The formal apparatus that still dominates this work—microeconomics, game theory, mechanism design, welfare economics, and social‑choice theory—was forged in, and for, the 20th century. We bundle these strands under the label *Standard Institution Design Toolkit (SIDT)*.

The SIDT idealises agents as *utility / preference maximisers*: each individual comes equipped with a complete, context‑independent ordering over outcomes. Over the last three decades researchers have proposed a wealth of refinements—menu‑dependent and **incomplete** preferences (Gul & Pesendorfer 2001; Bewley 2002), models of **social** or **other‑regarding** preferences (Fehr & Schmidt 1999), **behavioural** relaxations of expected‑utility axioms (Tversky & Kahneman 1992), and participatory-budgeting or quadratic-voting mechanisms that elicit richer information (Goel et al. 2019). These advances fix important *technical* bugs, yet they leave four *philosophical* limitations untouched:

### 2.1.1 Epistemic Limitations

The SIDT suffers from fundamental **opacity**: preferences remain private data with no shared representation that lets others audit, deliberate over, or contest them[^1]. Without an inspectable model of what people value and why, we cannot verify that AI systems have properly understood human intentions.

Preference-based approaches also cannot account for how human values actually function. People's preferences are often **incomplete**, **inconsistent**, and **unstable** over time. Utility theory lacks mechanisms to model agents who change, reshape, and discover preferences—much less agents that *reason* about which preferences or values are more sensible or justified to hold.

When scaled to collective contexts, this opacity creates accountability gaps. Representative agents cannot explain their reasoning about values, and there's no way to validate or challenge an AI's interpretation of what a community considers worth preserving or pursuing.

### 2.1.2 Autonomy Problems

The SIDT exhibits **a-normativity**: it stays neutral about *which* preferences are worth having and cannot distinguish authentic desires from manipulated ones[^2]. As established in welfare economics debates associated with Amartya Sen, revealed preference proves fundamentally limited as a measure of benefit.

Businesses, governments, and other entities have learned to exploit individuals under the guise of serving preferences, particularly through AI systems[^3]. As AI systems grow more sophisticated and pervasive, this manipulation intensifies. Current AI models actively engage in reward hacking[^4], and humans similarly "hack" themselves through behaviors that satisfy proximate preferences while undermining deeper values.

This creates a significant challenge: SIDT provides no principled way to distinguish authentic from manipulated preferences, considering humans to be "fulfilling their preferences" even when engaged in maladaptive behaviors like compulsive scrolling or AI-mediated addiction. A preference-maximizing model happily optimizes users into digital dependence because the revealed data say "they like it," creating inherent vulnerabilities in any system designed on these foundations.

### 2.1.3 Sociality Deficiencies

The SIDT exhibits **context-blindness**: utility is attached to solitary outcomes, not to the mesh of roles, narratives, and shared norms in which human action is embedded. This ignores how our values are fundamentally shaped by social context.

Game-theoretic approaches to cooperation—a cornerstone of the SIDT—fail to capture how humans actually cooperate. In classical solution concepts like Nash equilibria, agents maximize payoffs assuming independent and uncorrelated actions. This view of "rational" multi-agent interaction provides no framework for understanding genuine shared social norms.

Standard game theory has been repeatedly criticized for failing to predict cooperation in settings like the one-shot Prisoner's Dilemma, where humans frequently cooperate, and for its inability to explain humans' ability to make and maintain promises without external enforcement. The strategic rationality endorsed by conventional game theory is essentially Machiavellian—a rationality where deceptive promises are considered reasonable and coercive tactics are fair play.

This inadequacy extends to democratic institutions. Preference/utility frameworks in social choice theory fail to support sophisticated democratic processes where:

- Impacted individuals typically lack time to express detailed preferences about every possible outcome
- Representative agents cannot provide accountable reasoning
- Preference frameworks assume static preferences rather than capturing how preferences evolve through deliberation

The standard version of social choice misses the most powerful lever in democratic deliberation: inspiration. Effective democratic mechanisms should not merely aggregate existing preferences but facilitate the formation of new ones.

### 2.1.4 Aspirational Blindness

Perhaps most fundamentally, the SIDT assumes agents have fixed preferences disconnected from broader notions of the good. This prevents individuals, societies, and AI systems from aspiring to ideals beyond preference satisfaction.

Our social embeddedness—whether understood through shared values, norms, or beliefs—suggests forms of goodness beyond preference satisfaction: moral progress, enhanced cooperation across scales, and the discovery of deeper truths[^9]. As long as individual preferences remain the sole measure of good, these dimensions cannot be acknowledged.

While preference satisfaction metrics might show economic growth, other social goods appear to be declining: our capacity for collective knowledge production has deteriorated through misinformation; shared moral frameworks have weakened; and cooperation mechanisms have eroded across various domains. Preference-based metrics cannot detect these declines, nor would AI systems designed to maximize preference satisfaction address them.

### The Fundamental Misalignment

These four limitations point to a deeper problem: a fundamental misalignment between SIDT's model of human agency and how humans actually relate to values and norms. The ultimate test of any framework for human agency is how well the systems it informs *fit* human nature. Current mechanism design implies agents who optimize, calculate, strategize, and reduce values to numbers. These activities aren't foreign to human nature, but they don't capture its fullness.

Consequently, participating in institutions designed on these principles requires constant effortful adaptation. People must translate their rich value landscape into the narrow language of preferences and utilities, abandoning much of what gives those values meaning in the first place.

The SIDT achieved prominence because it offered mathematical expressiveness and parsimony. These theories were "good enough" for the institution design challenges of their era. However, contemporary AI alignment challenges require novel governance forms that the SIDT cannot adequately conceptualize or analyze. We need approaches that better reflect how humans actually relate to values, norms, and social coordination.