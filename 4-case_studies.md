# Application Areas for Full-Stack Alignment

To demonstrate the practical value of FSA, let's take on five representative socio-technical alignment challenges. In each case, we'll show how structured representations of norms and values offer novel solutions where preference/utility frameworks and naive value-representations fal short.

### 4.1 AI value‑stewardship agents
Modern assistants can whisper in our ear thousands of times a day. Unless they *guard* our richly‑textured ideals, they tend to nudge us toward the cheap dopamine that maximises watch‑time or clicks—a slide that researchers call **value collapse** (Stray et al. 2021). The danger is not malicious takeover but quiet homogenisation: assistants that reshape what we care about and then faithfully serve the flattened proxy.

| Socio-technical Challenge | Why Thin Models Break | Example FSA solution | Future FSA work |
|---|---|---|---|
| Stop assistants steering users into addictive spirals and shallow goals. | Revealed preferences are treated as ground truth (Gul & Pesendorfer 2001); free‑text constitutions drift under optimisation (Anthropic 2022). | Capture *constitutive attentional policies* and weave them into a **personal moral graph** (Klingefjord, Lowe & Edelman 2024) that users can endorse after reflection. | Values‑based RL plus *evaluative‑understanding* tests that probe whether agents truly grasp thick moral concepts.

### 4.2 Normatively competent agents
Self‑driving cars can obey traffic law yet still barge through four‑way stops; content‑moderation AIs ban reclaimed slurs while missing their social purpose. Such errors illustrate **normative incompetence**—fitting the letter of rules while shredding their spirit. To keep institutions intact, autonomous systems must *see* the living norms that humans follow and update with them.

| Socio-technical Challenge | Why Thin Models Break | Example FSA solution | Future FSA work |
|---|---|---|---|
| Track, honour and adapt to fluid social norms. | Pay‑off maximisers defect when incentives flip (Tversky & Kahneman 1992); plain‑language rules have no formal semantics. | Train on **norm‑augmented Markov games** that separate pay‑offs from norms (Oldenburg & Zhi‑Xuan 2024) and use *resource‑rational contractualism* to universalise decisions (Jara‑Ettinger 2023). | Benchmarks for rapid norm‑learning, context inference, and contestable reasoning traces.

### 4.3 Win‑win AI negotiation
LLM delegates already draft contracts, schedule freight, and explore peace settlements. Left to thin rationality they default to brinkmanship—recent labs found LLM agents escalate faster than humans in diplomacy games (Bai et al. 2023). We need negotiators that treat cooperation as more than a trick of incentives.

| Socio-technical Challenge | Why Thin Models Break | Example FSA solution | Future FSA work |
|---|---|---|---|
| Avoid Machiavellian escalation and exploitative deals. | Utility revelation makes promises cheap; open‑sourcing full code is infeasible (Oesterheld 2022) and partial‑preference schemes lack trust (Hyafil & Boutilier 2007). | **Value revelation**—share structured commitments that expose an agent’s *integrity* without revealing strategy. | Strategy‑proof revelation protocols and metrics that detect value fakery or bait‑and‑switch tactics.

### 4.4 A meaning‑preserving AI economy
Finance now executes micro‑trades unconnected to human use‑value; social‑media platforms profit by hijacking attention *(Orlowski 2020)*. These **human‑detached** and **human‑antagonistic** sectors flourish because markets optimise the signals they can see—money and clicks—not lived meaning. This misalignment is poised to grow massively in a near-future AI-powered economy.

| Socio-technical Challenge | Why Thin Models Break | Example FSA solution | Future FSA work |
|---|---|---|---|
| Redirect capital toward genuine human flourishing. | Manipulated demand still scores as welfare; fairness externalities ignored (Fehr & Schmidt 1999). | **Outcome‑based contracts** that route payouts to *measured flourishing* via value graphs, backed by dynamic‑contracting tools (Philippon 2015). | Fraud‑resistant meaning metrics and mechanisms that propagate incentives through supply chains.

### 4.5 Democratic regulation at AI speed
An AI agent can close a trans‑border land deal in milliseconds; the affected citizens hear days later. Democracy faces a **speed mismatch** across time‑scale, jurisdiction, and expertise. If governance cannot catch up, legitimacy unravels and human agency is undermined.

| Socio-technical Challenge | Why Thin Models Break | Example FSA solution | Future FSA work |
|---|---|---|---|
| Express the informed public will in real time across contexts. | Polling/voting is slow; abstract AI principles lack mandate; quadratic voting still needs richer value inputs (Goel et al. 2019). | Keep a **population‑scale moral graph** (Klingefjord et al. 2024) that LLM regulators cite in justifications and open for audit; combine with legitimation mechanisms (Conitzer 2023). | Algorithms for deliberative aggregation and reversible “fast‑path” oversight that can be appealed post‑hoc.
