# Research & Implementation Roadmap

We here coin the term "Full-Stack Alignment" to cover three things:

1. The provocative suggestion that many socio-technical alignment challenges can be made tractable by migrating from preference/utility frameworks to a new institution design toolkit based on explicit modeling of norms and values.

2. The unfortunate fact that, unless we simultaneously align AI models themselves and institutions at various levels of the stack, the incentives at one layer (say, geopolitics) will produce pressure to misalign the other layers (say, ruthless military AI).

3. A plan for research and societal transformation: to bring this work from basic research, through various legitimation steps, to policy proposals with broad support among experts and the public, and finally to broad implementation in new algorithms, institutions, and mechanisms—all in time to head off the socio-technical threats before AI agents cause widespread harms.

In what follows, we outline a research and implementation strategy for moving towards a society with social systems and institutions built on new frameworks for values and norms.

We argue that theoretical research alone in the five areas outlined in our case studies is not sufficient. For an AI lab to adopt a mechanism built on these new foundations, three key conditions must be met: 

1. Expert consensus must exist that the solution is the best available approach
2. A clear flagship implementation must exist as a concrete example
3. There must be widespread demand for the solution demonstrated by the flagship example

The absence of any one condition significantly reduces the likelihood of adoption. Expert consensus without implementation examples risks being dismissed as theoretically intractable. Clear flagship examples without public demand are likely deemed too risky for adoption. Public demand without flagship examples often fails to produce change (as illustrated by climate policy development, where public outcry without clear expert consensus and implementation pathways produced limited results).

Therefore, our general approach is to systematically move work on full-stack alignment for all of the five domains identified in our case studies from research to implementation.

## 5.1 Fundamental Research

The first phase focuses on developing rigorous theories, formalisms, and prototypes demonstrating the viability of explicit norm- and value-based institution design.

The goal of this research is to develop viable solutions to the full-stack alignment challenges identified in our case studies. This requires developing new techniques and formalisms, building proof-of-concept systems and mechanisms, and validating them through various legitimation tests—making formal claims about optimality and robustness while gathering data about small-scale deployments, user experience, and legitimacy.

## 5.2 Expert Consensus & Flagship Real-World Implementations

The second phase focuses on cultivating a strong interdisciplinary community—spanning scholars, technologists, and policymakers—to establish clear expert agreement around these novel frameworks, while launching targeted real-world pilots.

Once initial solutions appear viable, we need to build consensus among policymakers and decision-makers that this approach offers the best path forward. This involves creating advisory networks, facilitating frequent meetings between researchers and implementers, and developing demonstrations. Each solution will be distilled into concise policy or product recommendations supported by the relevant research.

In parallel, we will pursue opportunities for real-world deployments across our key domains:

- **Market interventions**: Working with exchanges or market infrastructure providers to implement value-aligned mechanisms in real-world settings, perhaps as pilot projects or within specific submarkets.

- **Democratic interventions**: Engaging with forward-looking jurisdictions (e.g., Estonia, Norway, Taiwan) prepared to pilot innovative approaches to AI governance, such as value-based AI regulatory frameworks.

- **Morally-competent agents**: Partnering with financial marketplaces or corporate suppliers willing to trial systems restricted to normatively competent AI agents.

These flagship implementations need not be deployed by the largest countries or top AI labs, but they must deliver robust successes that can serve as reference points for future development.

These real-world deployments will not only demonstrate the viability of FSA approaches but can also generate positive feedback cycles between implementation success and broader adoption.[^1]

[^1]: Successful implementations generate increased stakeholder demand, which in turn creates opportunities for more ambitious implementations across additional domains.
