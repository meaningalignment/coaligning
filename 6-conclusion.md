# Conclusion & Open Questions

This paper has presented Full-Stack Alignment (FSA) as a framework for addressing the socio-technical challenge of co-aligning AI systems and institutions. We have argued that two dominant paradigms—preference/utility maximization inherited from the Standard Institution Design Toolkit (SIDT) and prompt- or self-critique-based text alignment—are structurally inadequate for robust socio-technical alignment. In their place, we have proposed explicit, structured representations of human norms and values that can be inspected, verified, and deliberated over.

## Summary of Contributions

Our approach makes three primary contributions:

First, we have identified fundamental limitations in existing alignment paradigms. The preference-based approach fails to account for the instability of human preferences, their vulnerability to manipulation, and their inadequacy for modeling sophisticated cooperation. The text-based approach suffers from specification fragility, verification difficulties, and vulnerability to ideological pollution. Together, these limitations render current approaches insufficient for high-stakes alignment.

Second, we have developed a new toolkit centered on explicit norm and value representations. We outlined two fundamental approaches: (1) defining attractor points toward which values and norms naturally trend, such as Kantian equilibria and self-other generalizations, and (2) tightening the structure of what counts as a norm or value, as exemplified by constitutive attentional policies. These approaches enable us to distinguish genuine values from instrumental considerations, strategic concerns, and ideological markers.

Third, we demonstrated through five case studies how these structured representations can address previously intractable alignment challenges: preventing value collapse in personal AI assistants, developing normatively competent agents, enabling win-win AI negotiation, creating a meaning-preserving economy, and designing democratic institutions that can operate at AI speed.

## Priority Research Directions

Building on our case studies, we identify five concrete technical directions that warrant immediate investigation:

### Values-Based Reinforcement Learning

As highlighted in our AI value stewardship case study, we need reinforcement learning methods that optimize for constitutive values rather than instrumental preferences. This requires formalizing how structured value representations guide action selection and developing evaluative understanding in AI systems to comprehend thick evaluative concepts rather than merely manipulating them.

### Norm-Enriched Architectures

For normatively competent AI, we must extend standard reinforcement learning frameworks to incorporate structured norm representations. This includes developing multi-agent training environments where systems learn to identify and adapt to emergent norms, and implementing formal models of universalization and virtual bargaining that can guide AI decision-making.

### Values-Interpretable Negotiation Systems

To enable win-win AI negotiation, we need architectures whose values and commitments can be extracted and communicated in verifiable form. This requires designing strategy-proof revelation mechanisms that incentivize truthful value disclosure while preventing exploitation, and developing integrity verification methods to assess whether agents reliably act in accordance with their stated values.

### Value-Aligned Economic Mechanisms

For a meaning-preserving economy, we need robust, fraud-resistant methods for measuring meaningful human experiences across diverse domains. This includes designing market structures that efficiently allocate resources to activities that contribute to human flourishing and creating network representations of how economic activities relate to human values.

### Deliberative Value Aggregation

To enable democratic governance at AI speed, we must develop methods to combine individual value graphs into legitimate collective representations. This includes creating systems that generate auditable, contestable explanations for regulatory decisions grounded in population-level values and designing adaptive legitimation mechanisms that maintain democratic accountability for high-speed regulatory actions.

## Toward a New Institutional Paradigm

Full-Stack Alignment is not merely a technical project but an institutional one. It calls for a reconfiguration of the relationship between AI systems and human institutions—a reconfiguration that preserves and enhances human agency rather than diminishing it. By moving beyond the limitations of preference satisfaction and text-based alignment, FSA opens the possibility of AI systems that genuinely serve human flourishing.

The path forward will require close collaboration among technical researchers, social scientists, ethicists, policymakers, and the broader public. It will demand both theoretical advances and practical experimentation in real-world settings. But the potential reward is significant: a technological future where AI systems and human institutions co-evolve in ways that strengthen rather than undermine our collective capacity to realize our deepest values.

If successful, this approach may contribute not only to AI alignment but to an institutional renewal that addresses longstanding limitations in how we collectively organize to pursue human flourishing. The explicit, accountable representation of norms and values offers a foundation not just for aligning AI, but for reimagining human institutions in an age of unprecedented technological change.
