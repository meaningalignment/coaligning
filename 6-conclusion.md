# Conclusion & Open Questions

This paper has presented Full-Stack Alignment (FSA) as a framework for addressing the socio-technical challenge of co-aligning AI systems and institutions. We have argued that two dominant paradigms—preference/utility maximization inherited from the Standard Institution Design Toolkit (SIDT) and prompt- or self-critique-based text alignment—are structurally inadequate for robust socio-technical alignment. In their place, we have proposed explicit, structured representations of human norms and values that can be inspected, verified, and deliberated over.

## Summary of Contributions

Our approach makes three primary contributions:

First, we have identified fundamental limitations in existing alignment paradigms. The preference-based approach fails to account for the instability of human preferences, their vulnerability to manipulation, and their inadequacy for modeling sophisticated cooperation. The text-based approach suffers from specification fragility, verification difficulties, and vulnerability to ideological pollution. Together, these limitations render current approaches insufficient for high-stakes alignment.

Second, we have developed a new toolkit centered on explicit norm and value representations. We outlined two fundamental approaches: (1) identifying attractors in value space that help distinguish genuine values from mere preferences, such as Kantian equilibria and self-other generalizations, and (2) establishing formal constraints on the representation of values and norms, and on reasoning about them, as exemplified by constitutive attentional policies. These approaches enable us to distinguish genuine values from instrumental considerations, strategic concerns, and ideological markers.

Third, we demonstrated through five examples studies how these structured representations can address previously intractable alignment challenges: preventing value collapse in personal AI assistants, developing normatively competent agents, enabling win-win AI negotiation, creating a meaning-preserving economy, and designing democratic institutions that can operate at AI speed.

## Toward a New Institutional Paradigm

Full-Stack Alignment is not merely a technical project but an institutional one. It calls for a reconfiguration of the relationship between AI systems and human institutions—a reconfiguration that preserves and enhances human agency rather than diminishing it. By moving beyond the limitations of preference satisfaction and text-based alignment, FSA opens the possibility of AI systems that genuinely serve human flourishing.

The path forward will require close collaboration among technical researchers, social scientists, ethicists, policymakers, and the broader public. It will demand both theoretical advances and practical experimentation in real-world settings. But the potential reward is significant: a technological future where AI systems and human institutions co-evolve in ways that strengthen rather than undermine our collective capacity to realize our deepest values.

If successful, this approach may contribute not only to AI alignment but to an institutional renewal that addresses longstanding limitations in how we collectively organize to pursue human flourishing. The explicit, accountable representation of norms and values offers a foundation not just for aligning AI, but for reimagining human institutions in an age of unprecedented technological change.
