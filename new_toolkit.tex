\section{A New Toolkit for Alignment}
The advantage with preference relations or free-form text strings, is they can represent anything. Utility functions can represent any consistent set of choices, constructive or destructive; text strings can encode any instructions, cooperative or uncooperative. No assumptions get baked in about what values are, when they apply, or about the nature of the good (what kinds of attractor states values tend towards).

Yet that's also their chief drawback: an alignment target expressed this way can be pulled in any direction. It risks being polluted by considerations we would not, on reflection, recognise as values at all—signals we would not identify with any notion of the good.

This is one root cause of the problems with SIDT and the naive textual approaches catalogued in Section 2: it explains why addictive or manipulated behaviours can masquerade as bona-fide \textit{values}, why zero-sum brinkmanship is mistaken for ``optimal cooperation,'' why slogans like ``Defund the Police'' or ``Abortion is Murder'' might be read as instructions rather than tribal badges, and why vague aspirations such as ``AI should always do the right thing'' create an illusion of constraint while offering little guidance.

To overcome these limitations, we need a framework that takes a stance on what values and norms are, or are about, rather than treating all preference relations or text strings as equally valid candidates. In other words, we must say more about \textbf{normativity itself}.  There are two places to start: (a) We can identify patterns of \textbf{normative convergence} by exploring what values or norms are \textit{for}, what they're supposed to \textit{point at}, or how we'd \textit{recognize a good one}; or (b) alternatively, we can develop \textbf{structural representations} of the normative, finding \textit{explicit, structured encodings} of norms and values that resist drift in arbitrary directions.

\subsection{Approach 1: Normative Convergence}

One way to identify normative convergence is to imagine that norms or values are meant to fulfill a purpose, and have varying levels of fitness for that purpose. If values are \textit{for something}—if what's good is not arbitrary but advances in an identifiable direction—then we can distinguish authentic normative considerations from mere preferences, tribal markers, or passing fads based on their content.

This approach identifies 'attractors in value space'—organizing principles that help us understand what values are ultimately about, and provide criteria for recognizing which considerations might reliably re-emerge and stabilize (ideally, regardless of cultural or historical contingencies) and which should be considered as noise.

Here's an example: Velleman (1989, 2009) suggests that values emerge as common patterns of goodness abstracted across standpoints and contexts: considerations that remain beneficial across multiple perspectives become recognized as values, while merely instrumental concerns or temporary preferences don't achieve this stability. For example, a value like ``honesty'' tends to be recognized across different agents, contexts, and time periods. Recent research on self-other generalization (arXiv:2412.16325) is an example of fine-tuning work in this vein.

Another approach would be to treat values or norms as optimal solutions to recurring social cooperation problems. This is demonstrated in frameworks such as \textit{Kantian equilibria} (Roemer, 2010) or \textit{dependency equilibria} (Spohn, 2003; Treutlein, 2023), which move beyond standard game-theory approaches like Nash equilibria. These frameworks identify cooperation attractors where agents ask ``what if everyone like me acted this way?'' rather than merely maximizing personal outcomes. Certain cooperative norms thus appear as attractors in value space that stabilize in social interactions even without external enforcement.

Other candidate attractors are easy to come by: values could trend towards the auto-catalysis of flourishing among many diverse actors, where value-aligned actions create positive feedback leading to further flourishing. Or values could be considered as capability enhancing, increasing what agents can accomplish together beyond their individual limitations.

Whatever we imagine values are for, taking a stance can structure alignment much more clearly than the SIDT or naive text strings can.

\subsection{Approach 2: Structural Representation}

A complimentary approach is to establish formal constraints on how values and norms are represented and reasoned about, encoding values so they resist misinterpretation and manipulation, remain stable under the pressure of optimization processes, and don't drift toward arbitrary targets.

An example here is work on values as constitutive attentional policies. Instead of representing values associated with choices as utility functions, we can encode them as sets of criteria that agents intrinsically attend to when making decisions guided by that value (Klingefjord, Lowe, \& Edelman, 2024). This specification language distinguishes constitutive from instrumental considerations, excludes tribal affiliations and ideological markers, and represents the phenomenology of value-guided attention.

Another example is when norms are structured as formal constraint systems or filters that modify plans of action to ensure compliance. These systems specify boundary conditions that acceptable actions must satisfy, creating a clear demarcation between norm-compliant and norm-violating behavior. Such schemas protect alignment targets from pollution by arbitrary external goals or social pressures that cannot be properly characterized as legitimate norms.

Whichever you prefer, these representations can enable thick justificatory frameworks -- approaches to reasoning about values and actions that adopt more substantive accounts of normative justification. Choices are deemed reasonable insofar as they can be justified according to contextually-relevant criteria, such as:

Unlike the arbitrary interpretation paths of free-form text specifications discussed in section 2.2.2, structured representations unlock a plethora of formal reasoning approaches from philosophy, logic, and computer science. These include deontic logics for reasoning about obligations, defeasible reasoning frameworks for non-monotonic inference, formal argumentation systems, and theorem-proving methods that can provide verifiable guarantees about normative reasoning. With these formal tools, an AI system can be validated against specific criteria such as:

\begin{itemize}
\item Whether the choice promotes one's constitutive values
\item Whether it upholds the standards of one's social role
\item Whether it could be reasonably rejected by others affected by the choice
\end{itemize}

These structured representations can provide guardrails that keep normative reasoning on track, preventing it from being hijacked by optimization processes or strategic manipulation.

\subsection{Integration with Existing Methods}

Once we've established structural representations or identified patterns of normative convergence, there are options to reintegrate the results with the power mathematical formalisms of the SIDT and the recent successes of critique-based RL fine-tuning.

The major results of rational choice, game theory, and micro-economics assume utility functions that represent stable preference orderings. To capture richer values and norms, we can keep that mathematical scaffolding but enlarge the domain—``utility'' would no longer reflect mere preferability, but an all-things-considered value judgment.

\begin{itemize}
\item For instance, an enriched model of rational choice can account for how agents trade off norm compliance with their individual desires or objectives when taking actions, resulting in norm-augmented utility functions (Oldenburg \& Zhi-Xuan, 2024). This factoring would be useful for determining new norms that better promote each individual's interests, or for designing welfare functions that account for intrinsically-valued norms while factoring out oppressive norms.

\item Similarly, one can model how individuals make decisions based on values that are constitutive of their conception of a good life, weighted against considerations that are merely instrumental to some further value or goal (Edelman, 2022). These trade-offs can be captured by a utility function, but an institution or algorithm might prioritize constitutive values over instrumental ones.
\end{itemize}

RL-based training methods which condition on reasoning traces can also still be used, but instead of having a human annotator or an unprincipled expert model evaluate the reasoning, it can be evaluated more systematically via formal models of values-based reasoning.